LIMITATIONS AND THREATS TO VALIDITY:
There are a number of threats to the validity of our report which we realized late in our research.
First, we looked at the top rated answers overall. Based on the way that votes are distributed, it is likely that the pool of top rated answers is not distributed evenly across questions and many answers could have come from the same question. This means that there could have been a better written answer with lower visibility while our analysis instead took a less quality response because it had higher visibility.
We did not normalize our data set to remove duplicates (results that are both accepted and top rated). This means that the models we built may be more inaccurate because they were fed results that were identified as distinguishable, but in reality are not indistinguishable.
There’s an inherent limitation on score that must be considered and that we did not account for. Some questions will get more views and be more popular than others, and these questions will naturally draw more votes. Therefore, a well-constructed, correct answer to a highly popular question will score significantly higher than an answer of similar quality on a less popular question. This is a factor that is impossible for us or anybody else to account for, since question popularity is a highly variable and unpredictable factor and that information is also not contained in the data dump provided

CONCLUSIONS:
From our analysis, there are a number of conclusions that can be made. First, because of the relationship between classifier accuracy and score threshold of top answers, we demonstrated that there is no significant correlation between a Stack Overflow answer being accepted and an answer having a high score. This can be partially explained by the failure to remove duplicate answers from both the datasets; however, it stands to reason that questions with more traffic have multiple answers with high scores whereas only one high scoring answer per question can be selected. As for distinguishing highly rated and accepted answers, presence of code snippets and response time are decent predictors of accepted answers while presence of links is effectively useless as an indicator. It is interesting to note that there is even a difference in terms of content between these posts. From our model, we can see that question askers are relatively more interested in code and quick responses whereas the community is less inclined to include these features. Finally, as can be expected, words, adjectives, verbs, and nouns are highly correlated and are comparatively ineffective for analysis when used in unison. In finality, “what makes for a good answer?” is a difficult question to answer and really depends on who is asking. A question asker appears to want a quick reply with a code snippet – presumably an example or finished code. Both question askers and the community like to see links – presumably to learn more on the topic at hand.

FUTURE WORK:
This study can be improved in a number of ways. First, our study was limited in effectiveness by including duplicate results in the analysis. These could be removed to generate more accurate models. Secondly, a significant improvement could be made by normalizing the score of the answers based on the score of the question to quantify popularity. We believe that score may be directly related to the number of views that a question receives and therefore may have little effect on whether an answer is accepted or not.