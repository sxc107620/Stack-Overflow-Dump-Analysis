We used a machine learning classification technique known as Alternating Decision Tree (ADT) to find an optimal model for parsing our results. We tested a few classification techniques (Decision Stump, J48, Simple Logistic, Bayesian Regression), but ADT offered a few distinct advantages. First, it offered a tree-based graphical representation of the model. Secondly, it was able to process a larger dataset and ran much quicker than other tree-based approaches (a difference of about 15 minutes for our smallest dataset). Importantly, this speed and size increase had no discernable effect on model accuracy (typically less than 1%). For analysis, we used cross-validation with 10 folds. Testing the results of cross-validation versus percentage split offered little difference in terms of stated model accuracy.

Our first task was to look at how well we could predict whether an answer would be accepted or top using all of our metrics. What we found was that as the score threshold for answers decreased, the difference between top answers and accepted answers decreased. We felt that accepted answers and top rated answers diverge as score increases are significant â€“ why would a question asker not agree with the community? - And therefore we focused our efforts on the highest tier of top answers versus accepted answers. In this tier, there were 57,156 top scoring answers. Because our goal was to determine which factors can be used to differentiate accepted answers from top answers, we selected 57,156 accepted answers to match our top scoring answers and provide meaningful classification percentage results. Because there is no obvious way to narrow the field of accepted answers, the sample of 57,156 accepted answers were selected randomly from the accepted answer population.

Additionally, although we looked at 7 metrics initially, we found that 4 were strongly correlated and therefore narrowed these fields by using only one of these predictors for each data set. You can see the rather negligible results this had on results in the 1st tier section below.

a) Metrics
On the subject of metric dependency, running significance tests in R showed that the Noun, Verb, Adjective, Word and Response Time metrics are all statistically dependent across all answer sets under examination. However, this is a very reasonable result - not every word in a sentence can be a singular part of speech assuming proper grammar (Another reasonable assumption for highly voted and/or accepted answers), meaning that as the number of words under one part of speech go up, the others will naturally rise to fill out the sentences. Furthermore, longer answers simply take more time to write; therefore some level of dependence is to be expected. However, link and code snippet presence are independent of both each other and all other metrics under consideration, another logical result seeing that copy/pasting a hyperlink or code snippet can easily be done regardless of the rest of your answer.

b) 1st Tier Top (57156 answers) vs Accepted (57156 answers)
As discussed previously, we chose to narrow our set of predictors after correlation analysis. The impact this had on our results is negligible as seen in Table 1 below. The decrease in classification accuracy may be data set specific, or may reflect an insufficient weighting of the number of words by the ADT algorithm.
(Table 1)
For our classification testing, we chose to use chose to use 10-fold cross-validation in order to have a larger training and testing sample size as our sample size was relatively small for this test. This had a rather insignificant positive offset on our classification data. Assuming the negative difference seen in Table 1 is not due to chance, the positive difference associated with validation and the negative difference between variable pruning effectively cancels out.
(Table 2)
The model from our ADT, shown in graphical format in Figure 1, was able to correctly classify instances 70.64% of the time. The model incorrectly classified answers the remaining 29.36% of the time. A total number of 114312 instances were used to build and validate the model.
(Figure 1)
Using our 4 metrics, we looked at the impact each metric had on predicting whether an answer is an accepted answer. Table 3 and Table 4 contain the most relevant information we collected. Notable outtakes from the tables are that number of words had very little impact on accurate prediction (~6%) and presence of links had almost zero impact (~0.5%). Conversely, presence of code had the highest single variable impact (~17%) with response time also having a noticeable impact (~14%).
(Table 3)
(Table 4)
c) 2nd Tier Top (98194 answers) vs Accepted (98194 answers)
The model from our ADT for the second tier of top answers versus accepted answers, shown in graphical format in Figure 2, was able to correctly classify instances 62.99% of the time. The model incorrectly classified accepted answers the remaining 37.01% of the time. A total number of 196388 instances were used to build and validate the model. The accuracy of this model is much less significant than the highest tier of top answers (a drop of ~6%).
(Figure 2)
d) 3rd Tier Top (225000 answers) vs Accepted (225000 answers)
The model from our ADT for the second tier of top answers versus accepted answers, shown in graphical format in Figure 2, was able to correctly classify instances 58.59% of the time. The model incorrectly classified accepted answers the remaining 41.41% of the time. A total number of 425000 instances were used to build and validate the model; this number was rounded down due to hard memory constraints of the computer we were running tests on. The number of records was relatively small compared to the sample space (14446).The accuracy of this model is even further reduced compared to previous models (a drop of ~4.5% from the 2nd tier and ~12% from the 1st tier).
(Figure 3)